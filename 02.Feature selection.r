library(tidyverse)# Data manipulation
library(ggplot2) # Plotting
#install.packages("ggvenn")
library(ggvenn)    # Venn diagram
library(pROC)      # ROC curve analysis
library(RColorBrewer) # Color palettes for plots
library(caret)     # Data splitting and cross-validation
library(randomForest) # Random Forest
library(glmnet)    # Lasso regression
#install.packages("xgboost")
library(xgboost)   # XGBoost
library(ggplot2)
library(viridis)
library(ggthemes)
library(ggsci)#############################################################################
#############################################################
df_yuan <- df # Backup of original df
data <- df1[,-c(1,3,4,29,30,37,39,41,42,44,45,48,50,52,55:57,59:67,77,78)]
data<- rename(data, Townsend_deprivation = 'Townsend deprivation')
data <- data[,c(34,1:9,46:47,10:33,35:45)]
# Variable factorization, factorize the outcome variable
data$PAD_new = factor(data$PAD_new,levels = c(0,1),labels = c('No','Yes'))    #Result为分类结局
#### Multiple machine learning methods for feature selection
#### Lasso regression
## Load packages and specify their functions
library(glmnet) ##Lasso regression
library(rms)  ## Nomogram plotting
library(VIM) ##aggr() function in this package is used to assess missing data
library(survival) ##Survival analysis package
library(tidyverse)
set.seed(520)
inTrain = createDataPartition(y = data[,"PAD_new"], p = 0.6, list = F)
traindata = data[inTrain,]         # Training set (original)
testdata = data[-inTrain,]         # Test set (original)

dev = traindata                    # Called 'dev' (development/training set)
vad = testdata                     # Called 'vad' (validation/test set)
table(dev$PAD_new, useNA="ifany")
#平衡训练集数据
# newData = SMOTE(PAD_new~ ., 
#                 dev, 
#                 k=5, # Number of nearest neighbors for generating minority class instances
#                 perc.over = 100, # Percentage to oversample minority class (e.g. 130 means 130%)
#                 perc.under= 200) # Percentage to undersample majority class for each generated case
# dev = newData
table(dev$PAD_new)
data <- dev
###############################################################################
##### Method 1: Lasso for feature selection
#######################################################################
# Use for loop to convert numeric variables to factor
#for(i in names(data)[c(2:10)]) {data[,i] <- as.factor(data[,i])}
# glmnet package only accepts matrix data; conversion required
# Convert categorical variables to dummy matrix
lassodata= data 
lassodata = as.data.frame(lapply(lassodata, function(x) {
  if(is.factor(x)) {
    as.numeric(x)
  } else {
    x
  }
}))

set.seed(43)
lassox <- model.matrix(~ . -1, data = data[, 2:ncol(data)])
data$PAD_new <- ifelse(data$PAD_new == "Yes", 1, 0)
lassoy <- as.numeric(as.character(data[,1]))
str(lassoy)
summary(lassoy)
table(lassoy, useNA="ifany")
# First approach: weights are set by inverse ratio
weight0 <- 1
weight1 <- sum(lassoy == 0) / sum(lassoy == 1)
weights <- ifelse(lassoy == 1, weight1, weight0)
fit.lasso<-glmnet(lassox, lassoy, family="binomial",alpha=1,
                  weights = weights,
                  maxit = 1e+05) 
cv.lasso<- cv.glmnet(lassox, lassoy, alpha = 1,                     
                     family = "binomial",                     
                     type.measure = "deviance",                     
                     nfolds = 10)
plot.data <- data.frame(as.matrix(fit.lasso$lambda),
                        as.matrix(t(fit.lasso$beta)))
#### Naming and reshaping plotting dataframe
plot.data%>%
  rename(lambda='as.matrix.fit.lasso.lambda.')%>%#rename columns
  pivot_longer(cols= 2:ncol(plot.data),#specify columns to reshape
               names_to = "variable",#name for new variable column
               values_to= "coef") ->plot.data#name for new coefficient value column
view(plot.data)
# Draw the plot
# Colors generated by viridis palette
library(viridisLite)
colors <- viridis(60)
### Custom function for scientific notation conversion
scientific_10 <- function(x) {
  parse(text=gsub("e", " %*% 10^", scales::scientific_format()(x)))
}
p1 <- ggplot(plot.data,
             aes(x=lambda,
                 y=coef,
                 color=variable))+
  geom_line(size=1,alpha=0.8)+
  scale_color_manual(values =colors)+
  scale_x_log10(label=scientific_10)+
  labs(title = "LASSO Regression Path",
       x = "Log lambda",
       y="Coefficient")+
  theme_bw()+
  theme(legend.text = element_text(size = 6))
p1
p2 <- plot(cv.lasso)
#### Extract minimal model parameters and coefficients
se_lambda <- cv.lasso$lambda.min  # minimum λ value with one standard error
se_lambda
# Regression coefficients at minimum λ value
#se_coef <- coef(cv.lasso,s="lambda.min") ## coefficients at λ = minimal value of one standard error
se_coef <- coef(cv.lasso,s=0.0006718755)
se_coef
index <- which(se_coef!=0)
index
coef <- se_coef[index][-1]
coef
# Database of variables with non-zero coefficients
diffvariables = row.names(se_coef)[index][-1] # non-zero variables
diffvariables
# Dataframe of non-zero variables & their coefficients
lasso.result.se <- cbind(diffvariables,coef)%>%
  as.data.frame()
lasso.result.se

lasso.result.se%>%
  mutate(direction=ifelse(coef>0,"up","down")) ->lasso.result.se
lasso.result.se$coef<-as.numeric(lasso.result.se$coef)
lasso.result.se
lasso.result.se$coef <- scale(lasso.result.se$coef, center = FALSE, scale = max(abs(lasso.result.se$coef)))
lasso.result.se
lasso.result.se%>%
  mutate(direction=ifelse(coef>0,"up","down")) ->lasso.result.se
lasso.result.se$coef<-as.numeric(lasso.result.se$coef)
lasso.result.se
lasso.result.se$coef <- scale(lasso.result.se$coef, center = FALSE, scale = max(abs(lasso.result.se$coef)))
lasso.result.se
lasso <- ggplot(lasso.result.se,
                aes(y=reorder(diffvariables,coef),
                    x=coef,
                    fill = direction))+
  geom_col(alpha=0.8,width = 0.6)+
  scale_x_continuous(limits=c(-0.3, 1), breaks=c(-0.1, 0, 0.1, 0.2,0.3))+
  labs(title = "Important Varibles in LASSO",
       x="Coef",
       y="")+
  coord_flip()+
  scale_fill_brewer(palette = "Set2")+
  theme_bw()+
  theme(axis.text.x = element_text(angle = 30,hjust = 0.9),
        legend.position = "")
library(ggpubr)
lasso1 <- ggbarplot(lasso.result.se, x="diffvariables", y="coef", fill = "direction", color = "white", 
                    orientation = "horiz",   # horizontal display
                    palette = "nejm",        # color schemes, also npg, aaas, jama, jco
                    legend = "right",        # legend position
                    sort.val = "asc",        # reverse order, or desc for normal order
                    sort.by.groups=TRUE)+
  theme_few()
#install.packages("ggbreak")
library(ggbreak)
lasso2<-lasso1+scale_y_break(c(-0.07,-0.015), # scale break locations/ranges
                             space = 0.2,     # gap size
                             scales = 5)+     # display proportion, >1 larger above, <1 larger below
  scale_y_break(c(0.004,0.17), # scale break locations/ranges
                space =0.1,    # gap size
                scales =3)+
  scale_y_continuous(breaks =c(-0.3,-0.01, 0,0.2,1))
lasso2
#scale_y_continuous(limits=c(-1, 1), breaks =c(-0.02, -0.01, 0.0, 0.1,0.2))
lasso2<- lasso2 +
  labs(title = "Top 28 Importance Variable Plot of lasso", x = "Variable", y = "Importance")+
  theme(
    plot.title = element_text(hjust = 0.5,size = 12,face="bold"),
    axis.text=element_text(size=9,face="bold",color="black"),
    legend.title = element_blank(),
    legend.text = element_text(size=9,face="bold"),
    #legend.position=c(0.65,0.30),
    legend.background = element_blank(),
    axis.text.y = element_text( hjust=1, size=12, face="bold",color="black"),#element_blank()
    axis.title.x = element_text(size=9,face="bold",color="black"),
    panel.border = element_rect(color="black",size=1),
    panel.background = element_blank())

#########################################################################################
## Method 2: Boruta for feature selection
#install.packages("Boruta")
# 1. Load packages
library(DMwR)       # SMOTE
library(Boruta)     # Boruta feature selection
library(ranger)     # ranger random forest

# 2. Custom rangerImp function (Boruta defaults to randomForest, can use ranger for speed)
rangerImp <- function(x, y, ...) {
  rf <- ranger::ranger(dependent.variable.name = ".outcome",
                       data = data.frame(x, .outcome = y),
                       importance = "impurity", ...)
  rf$variable.importance
}

# 3. Data input, assume 'data' dataframe and 'PAD_new' as target variable
# Ensure PAD_new is factor (SMOTE only supports categorical)
data$PAD_new <- as.factor(data$PAD_new)

# 4. Balance data with SMOTE
set.seed(123)   # Set seed for reproducibility
data_balanced <- SMOTE(PAD_new ~ ., data = data, perc.over = 100, perc.under = 200)
# perc.over=100 means oversample minority class by 100% (generate equal number as original)
# perc.under=200 means undersample majority class by 200%
# Adjust percentage parameters based on sample ratio

# 5. Remove constant columns, NA, Inf problematic columns for Boruta
feature_names <- setdiff(names(data_balanced), "PAD_new")
X <- data_balanced[, feature_names]
y <- data_balanced$PAD_new

feature_filter <- sapply(X, function(col) {
  vals <- col[!is.na(col) & !is.infinite(col)]
  length(unique(vals)) > 1
})
X <- X[, feature_filter, drop = FALSE]

# 6. Boruta for feature selection
boruta_result <- Boruta(
  x = X,
  y = y,
  doTrace = 2,
  getImp = rangerImp,
  ntree = 200,
  maxRuns = 100
)

# 7.  Visualization of results
plot(boruta_result, las = 2, cex.axis = 0.7)

# 8. Output confirmed important variables
confirmed_vars <- getSelectedAttributes(boruta_result, withTentative = FALSE)
print("Confirmed important variables:")
print(confirmed_vars)

library(Boruta)
library(ggplot2)
library(dplyr)
library(tidyr)

# Get Boruta variable importance stats
boruta_stats <- attStats(boruta_result)
boruta_stats$variable <- rownames(boruta_stats)
boruta_stats$decision <- factor(boruta_stats$decision, 
                                levels = c("Confirmed","Tentative","Rejected"))
boruta_stats <- boruta_stats %>%
  arrange(decision, desc(medianImp)) %>%
  mutate(variable = factor(variable, levels = variable))

ggplot(boruta_stats, aes(x=variable, y=medianImp, fill=decision)) +
  geom_bar(stat="identity", color="black", width=0.8) +
  scale_fill_manual(values=c("Confirmed"="#30B84A",   # 明亮绿
                             "Tentative"="#FFC857",   # 橙色
                             "Rejected"="grey85")) +
  theme_minimal(base_size = 16) +
  labs(
    title = "Boruta Feature Importance",
    x = "",
    y = "Median Importance (Z-score)"
  ) +
  theme(
    axis.text.x = element_text(angle=60, hjust=1, size=10, face="bold"),
    legend.title = element_blank(),
    plot.title = element_text(hjust = 0.5, face="bold")
  )
boruta_stats %>% filter(decision != "Rejected") -> boruta_stats_top

ggplot(boruta_stats, aes(x=variable, y=medianImp, fill=decision)) +
  geom_bar(stat="identity", color="black", width=0.8) +
  geom_text(aes(label=round(medianImp)), vjust=0.5,hjust=1.1, 
            size=3, angle=90, fontface = "bold") +
  scale_fill_manual(values=c("Confirmed"="#2171b1",
                             "Tentative"="#30B84A",
                             "Rejected"="grey85")) +
  theme_few(base_size = 12) +
  labs(
    title = "Boruta Feature Importance",
    x = "",
    y = "Median Importance (Z-score)"
  ) +
  theme(
    axis.text.x = element_text(angle=60, hjust=1, size=12, face="bold"),
    legend.title = element_blank(),
    plot.title = element_text(hjust = 0.5, face="bold",colour = "black")
  )

##########
# Keep all Confirmed, Tentative, and top 2 Rejected variables
n_rejected_show <- 2

# Filter each category
boruta_stats_confirmed <- boruta_stats %>% filter(decision == "Confirmed")
boruta_stats_tentative <- boruta_stats %>% filter(decision == "Tentative")
boruta_stats_rejected  <- boruta_stats %>%
  filter(decision == "Rejected") %>%
   head(n_rejected_show) # keep top 2 Rejected

# Combine
boruta_stats_plot <- bind_rows(
  boruta_stats_confirmed,
  boruta_stats_tentative,
  boruta_stats_rejected
)

# Change factor ordering for x axis
boruta_stats_plot$variable <- factor(boruta_stats_plot$variable, levels = boruta_stats_plot$variable)

# Plot
ggplot(boruta_stats_plot, aes(x=variable, y=medianImp, fill=decision)) +
  geom_bar(stat="identity", color="black", width=0.8) +
  geom_text(aes(label=round(medianImp)), vjust=0.5,hjust=1.1, 
            size=3, angle=90, fontface = "bold") +
  scale_fill_manual(values=c("Confirmed"="#2171b1",
                             "Tentative"="#30B84A",
                             "Rejected"="grey85")) +
  theme_few(base_size = 12) +
  labs(
    title = "Boruta Feature Importance",
    x = "",
    y = "Median Importance (Z-score)"
  ) +
  theme(
    axis.text.x = element_text(angle=75, hjust=1, size=12, face="bold",colour = "black"),
    legend.title = element_blank(),
    plot.title = element_text(hjust = 0.5, face="bold",colour = "black")
  )

# Extract Boruta selected variables and save
# Brouta_data = alldata[,c("Result",lassovar)]
# write.csv(Brouta_data,"Brouta_select_data.csv",row.names = F)

##### mRMR
#install.packages("mRMRe")
# Required packages
library(mRMRe)
library(corrplot)

# Step 1: Copy and convert all to numeric, avoid mRMRe error
mrmr_feature <- as.data.frame(lapply(data, function(x) {
  if (is.numeric(x)) return(x)
  # Convert factors/characters to numeric (map 0/1 to 1/2 etc.)
  if (is.factor(x) | is.character(x)) return(as.numeric(as.character(x)))
  return(as.numeric(x))
}))

# Step 2: Check for missing/inf, remove problematic columns
valid_cols <- sapply(mrmr_feature, function(x) all(!is.na(x)) & all(!is.infinite(x)))
mrmr_feature <- mrmr_feature[, valid_cols]

# Step 3: Define target variable index
target_index <- which(names(mrmr_feature) == "PAD_new")
# 1. Force numeric per column
mrmr_feature <- as.data.frame(lapply(data, function(x) as.numeric(as.character(x))))
# 2. Remove all NA columns
mrmr_feature <- mrmr_feature[, colSums(is.na(mrmr_feature)) == 0]

# 3. Build mRMR data object
Data <- mRMR.data(data = mrmr_feature)

# Step 4: mRMR feature selection (select 20, adjustable)
Data <- mRMR.data(data = mrmr_feature)
set.seed(123)
mrmr_result <- mRMR.ensemble(data = Data, target_indices = target_index,
                             feature_count =40, solution_count =1)

# Step 5: Get feature indices and names
selected_index <- mrmr_result@filters[[as.character(mrmr_result@target_indices)]]
feature_selected <- names(mrmr_feature)[selected_index]
cat("Selected important features:\n")
print(feature_selected)
cor_mat <- cor(mrmr_feature[, feature_selected])
threshold <- 0.7
remove_idx <- c()
for(i in 1:(length(feature_selected)-1)) {
  for(j in (i+1):length(feature_selected)) {
    if(abs(cor_mat[i, j]) > threshold) remove_idx <- c(remove_idx, j)
  }
}
feature_selected_final <- feature_selected[unique(setdiff(1:length(feature_selected), remove_idx))]
print(feature_selected_final)
target_var <- "PAD_new"  # Target variable name
cor_to_target <- cor(mrmr_feature[, feature_selected_final], mrmr_feature[[target_var]], use = "pairwise.complete.obs")

barplot(
  cor_to_target[order(abs(cor_to_target), decreasing=TRUE)],
  horiz = TRUE,
  las = 1,
  col = "skyblue",
  main = paste(target_var, "Pearson correlation coefficients with selected features"),
  xlab = "Pearson correlation coefficient"
)
library(ggplot2)
library(ggsci)

# Assume mrmr_feature, feature_selected_final, PAD_new are defined
target_var <- "PAD_new"
cor_to_target <- cor(mrmr_feature[, feature_selected_final],
                     mrmr_feature[[target_var]], 
                     use = "pairwise.complete.obs")

dat <- data.frame(
  feature = feature_selected_final,
  cor = cor_to_target
)
# Custom ordering (optionally manually adjust or randomize for better visualization)
set.seed(21)
dat <- dat[order(dat$cor), ]
# Optional: manually reorder for improved visualization
dat$feature <- factor(dat$feature, levels = dat$feature)

# NEJM color scheme: negative values use first color, positive use fourth
nejm_col <- pal_nejm("default")(4)
dat$color <- ifelse(dat$cor >=0, nejm_col[2], nejm_col[1])

p <- ggplot(dat, aes(x=feature, y=cor, fill=color)) +
  geom_bar(stat="identity", width=0.7) +
  geom_text(aes(label=sprintf("%.2f", cor)), 
            hjust=ifelse(dat$cor>=0, -0.15, 1.15),  # 数值在条形内侧或条形外侧
            vjust=0.5, size=4, color="black") +
  scale_fill_identity() +
  coord_flip() +
  labs(
    title = "Pearson Correlation Coefficient with PAD_new",
    x = "Feature",
    y = "Pearson's r"
  ) +
  theme_few(base_size=15) +
  theme(
    plot.title=element_text(face="bold", size=16, hjust=0.5)
  )+
  theme(
    axis.text.y = element_text( hjust=1, size=12, face="bold",color="black"),
    legend.title = element_blank())

print(p)
##################
# -------------------- Automatic recognition -----------------------
# Robust code to prevent type confusion
lasso_vars <- as.character(na.omit(lasso.result.se$diffvariables))
lasso_vars <- unique(gsub("(0|1)$", "", lasso_vars))
print(lasso_vars)
boruta_vars <- as.character(na.omit(getSelectedAttributes(boruta_result, withTentative = FALSE)))
mrmr_vars <- as.character(na.omit(feature_selected_final))

feature_list <- list(
  LASSO = lasso_vars,
  Boruta = boruta_vars,
  mRMR = mrmr_vars
)

library(ggVennDiagram)

main_colors <- c("#E64B35", "#4DBBD5", "#00A087")  # red, blue, green, or your choice
p_venn <- ggVennDiagram(
  feature_list,
  label_alpha =1,
  #fill_color = main_colors
  #label = "count",            # or "percent"—keep your preferred content
  label_geom = "text",       # no background for text label
  label_color = "black"      # label font color set as black
) +
  theme(text = element_text(size=14,face="bold"),
        plot.title = element_text(hjust=0.5, face="bold")) +
  labs(title="Feature Selection Venn Diagram: LASSO, Boruta, mRMR")+
  scale_fill_gradient(low="#FFFFFF",high = "#b9292b",name = "variables")
print(p_venn)

cat("Intersection (features selected by all methods):\n")
venn_inter <- Reduce(intersect, feature_list)
print(venn_inter)

cat("\nUnion (all features selected by any method):\n")
venn_union <- unique(unlist(feature_list))
print(venn_union)

cat("\nFeatures unique to LASSO:\n")
print(setdiff(lasso_vars, union(boruta_vars, mrmr_vars)))
cat("\nFeatures unique to Boruta:\n")
print(setdiff(boruta_vars, union(lasso_vars, mrmr_vars)))
cat("\nFeatures unique to mRMR:\n")
print(setdiff(mrmr_vars, union(lasso_vars, boruta_vars)))
#install.packages("VennDiagram")
library(VennDiagram)

# Specify independent color for each circle
venn.plot <- venn.diagram(
  x = list(
    LASSO = lasso_vars,
    Boruta = boruta_vars,
    mRMR = mrmr_vars
  ),
  category.names = c("LASSO", "Boruta", "mRMR"),
  fill = c("#E98D83", "#95B6EC", "#D2EBD2"),   # red/blue/green, adjustable
  alpha = 0.5,                # circle transparency
  cex = 1,                    # font size for numbers
  fontfamily = "sans",        # font for numbers
  cat.cex = 1.5,              # font size for category names
  cat.fontfamily = "sans",
  cat.col = c("#CA645F", "#6295D7", "#9CAD8B"), # legend font color
  main = "Feature Selection Venn Diagram: LASSO, Boruta, mRMR",
  filename = NULL
)
grid.draw(venn.plot)

#####################
colnames(data)
save.image(file = "last.RData")
